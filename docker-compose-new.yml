version: '3.8'

services:
  # ============================================================================
  # CORE SERVICES - Self-Evolving AI Organism (8 Services)
  # ============================================================================
  
  # 1. Supreme Brain - Core Consciousness (Port 5010)
  supreme-brain:
    build:
      context: ./core/supreme-brain
      dockerfile: Dockerfile
    ports:
      - "5010:5010"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - DATABASE_URL=postgresql://orchestrator:${POSTGRES_PASSWORD}@postgres:5432/orchestrator_db
    depends_on:
      - postgres
      - redis
    networks:
      - ai-network
    restart: unless-stopped
  
  # 2. Agent Ecosystem - 2000+ AI Agents (Port 5011)
  agent-ecosystem:
    build:
      context: ./core/agent-ecosystem
      dockerfile: Dockerfile
    ports:
      - "5011:5011"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - RAY_ADDRESS=auto
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: "4"
          memory: "8G"
    depends_on:
      - redis
    networks:
      - ai-network
    restart: unless-stopped
  
  # 3. Tech Scouting Engine (Port 5020)
  tech-scouting:
    build:
      context: ./tech-scouting
      dockerfile: Dockerfile
    ports:
      - "5020:5020"
    environment:
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - QDRANT_URL=http://qdrant:6333
    depends_on:
      - qdrant
    networks:
      - ai-network
    restart: unless-stopped
  
  # 4. Hyper-Learning System (Port 5030)
  hyper-learning:
    build:
      context: ./hyper-learning
      dockerfile: Dockerfile
    ports:
      - "5030:5030"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - CHROMADB_HOST=chromadb
      - CHROMADB_PORT=8000
    volumes:
      - ./data/learning:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - chromadb
    networks:
      - ai-network
    restart: unless-stopped
  
  # 5. Media Factory Intelligence (Port 5012)
  media-factory:
    build:
      context: ./intelligence-layers/media-factory
      dockerfile: Dockerfile
    ports:
      - "5012:5012"
    environment:
      - REPLICATE_API_KEY=${REPLICATE_API_KEY}
      - ELEVENLABS_API_KEY=${ELEVENLABS_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./data/media:/app/outputs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - ai-network
    restart: unless-stopped
  
  # 6. Marketing Brain Intelligence (Port 5013)
  marketing-brain:
    build:
      context: ./intelligence-layers/marketing-brain
      dockerfile: Dockerfile
    ports:
      - "5013:5013"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    networks:
      - ai-network
    restart: unless-stopped
  
  # 7. Simulation Universe (Port 5040)
  simulation-universe:
    build:
      context: ./simulation-universe
      dockerfile: Dockerfile
    ports:
      - "5040:5040"
    privileged: true  # For Docker-in-Docker
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - ai-network
    restart: unless-stopped
  
  # 8. Cost Guard (Port 5050)
  cost-guard:
    build:
      context: ./cost-guard
      dockerfile: Dockerfile
    ports:
      - "5050:5050"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      - redis
      - vllm
    networks:
      - ai-network
    restart: unless-stopped
  
  # ============================================================================
  # EXISTING SERVICES (For compatibility)
  # ============================================================================
  
  # API Gateway - Main entry point
  api-gateway:
    build:
      context: ./services/api-gateway
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=info
      - SUPREME_BRAIN_URL=http://supreme-brain:5010
      - AGENT_ECOSYSTEM_URL=http://agent-ecosystem:5011
    depends_on:
      - supreme-brain
      - agent-ecosystem
      - redis
    networks:
      - ai-network
    restart: unless-stopped
  
  # ============================================================================
  # INFRASTRUCTURE SERVICES
  # ============================================================================
  
  # PostgreSQL + TimescaleDB for time-series data
  postgres:
    image: timescale/timescaledb:latest-pg15
    environment:
      - POSTGRES_DB=orchestrator_db
      - POSTGRES_USER=orchestrator
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    ports:
      - "5432:5432"
    networks:
      - ai-network
    volumes:
      - postgres-data:/var/lib/postgresql/data
    restart: unless-stopped
  
  # Redis / Dragonfly for caching and task queue
  redis:
    image: docker.dragonflydb.io/dragonflydb/dragonfly:latest
    ports:
      - "6379:6379"
    networks:
      - ai-network
    volumes:
      - redis-data:/data
    restart: unless-stopped
  
  # Qdrant for vector similarity search
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    networks:
      - ai-network
    volumes:
      - qdrant-data:/qdrant/storage
    restart: unless-stopped
  
  # ChromaDB for vector store (RAG)
  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8001:8000"
    networks:
      - ai-network
    volumes:
      - chromadb-data:/chroma/chroma
    restart: unless-stopped
  
  # vLLM for local inference (LLaMA 3 70B)
  vllm:
    image: vllm/vllm-openai:latest
    command: --model meta-llama/Meta-Llama-3-70B-Instruct --quantization awq --dtype auto
    ports:
      - "8000:8000"
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - ai-network
    volumes:
      - vllm-cache:/root/.cache/huggingface
    restart: unless-stopped
  
  # ElasticSearch for logging
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    networks:
      - ai-network
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    restart: unless-stopped
  
  # Kibana for visualization
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    networks:
      - ai-network
    restart: unless-stopped
  
  # Prometheus for metrics
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - ai-network
    restart: unless-stopped
  
  # Grafana for dashboards
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - ai-network
    restart: unless-stopped

networks:
  ai-network:
    driver: bridge

volumes:
  postgres-data:
  redis-data:
  qdrant-data:
  chromadb-data:
  vllm-cache:
  elasticsearch-data:
  prometheus-data:
  grafana-data:
